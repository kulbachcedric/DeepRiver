<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">0</article-id>
<article-id pub-id-type="doi">N/A</article-id>
<title-group>
<article-title>DeepRiver: A Deep Learning Library for Data
Streams</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9363-4728</contrib-id>
<name>
<surname>Kulbach</surname>
<given-names>Cedric</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2886-1219</contrib-id>
<name>
<surname>Cazzonelli</surname>
<given-names>Lucas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7583-753X</contrib-id>
<name>
<surname>Ngo</surname>
<given-names>Hoang-Anh</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1464-4520</contrib-id>
<name>
<surname>Halford</surname>
<given-names>Max</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0092-3572</contrib-id>
<name>
<surname>Mastelini</surname>
<given-names>Saulo Martiello</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>FZI Research Center for Information Technology, Karlsruhe,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>AI Institute, University of Waikato, Hamilton, New
Zealand</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>University of Sao Paulo, Brazil</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<volume>¿VOL?</volume>
<issue>¿ISSUE?</issue>
<fpage>¿PAGE?</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>1970</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Machine learning algorithms enhance and expedite decision-making
  processes based on available data. However, as data evolves over time,
  it becomes crucial to adapt machine learning (ML) systems
  incrementally to accommodate new data patterns. This adaptation is
  achieved through online learning or continuous ML technologies.
  Although deep learning technologies have demonstrated outstanding
  performance on predefined datasets, their application to online,
  streaming, and continuous learning scenarios has been limited.</p>
  <p><monospace>DeepRiver</monospace>(<xref alt="GitHub - Online-Ml/Deep-River — Github.com" rid="ref-githubGitHubOnlinemldeepriver" ref-type="bibr"><italic>GitHub
  - Online-Ml/Deep-River — Github.com</italic></xref>) is a Python
  package for deep learning on data streams. Built on top of
  River(<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
  et al., 2021</xref>) and
  PyTorch(<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>), it offers a unified API for both supervised and
  unsupervised learning. Additionally, it provides a suite of tools for
  preprocessing data streams and evaluating deep learning models.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In today’s rapidly evolving landscape, machine learning (ML)
  algorithms play a pivotal role in shaping decision-making processes
  based on available data. The acceleration facilitated by these
  algorithms necessitates constant adaptation to evolving data
  structures, as what is valid at one moment may no longer hold in the
  future. To address this imperative, adopting online learning and
  continuous ML technologies becomes paramount. While deep learning
  technologies have demonstrated exceptional performance on static,
  predefined datasets, their application to dynamic and continuously
  evolving data streams remains underexplored. The absence of widespread
  integration of deep learning into online, streaming, and continuous
  learning scenarios hampers the full potential of these advanced
  algorithms in real-time decision-making
  (<xref alt="Kulbach et al., 2024" rid="ref-kulbach2024retrospectivetutorialopportunitieschallenges" ref-type="bibr">Kulbach
  et al., 2024</xref>). The emergence of the
  <monospace>DeepRiver</monospace>(<xref alt="GitHub - Online-Ml/Deep-River — Github.com" rid="ref-githubGitHubOnlinemldeepriver" ref-type="bibr"><italic>GitHub
  - Online-Ml/Deep-River — Github.com</italic></xref>) Python package
  fills a critical void in the field of deep learning on data streams.
  Leveraging the capabilities of
  River(<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
  et al., 2021</xref>) and
  PyTorch(<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>), <monospace>DeepRiver</monospace> offers a
  unified API for both supervised and unsupervised learning, providing a
  seamless bridge between cutting-edge deep learning techniques and the
  challenges posed by dynamic data streams. Moreover, the package equips
  practitioners with essential tools for data stream preprocessing and
  the evaluation of deep learning models in dynamic, real-time
  environments. This was already made use of in the context of Streaming
  Anomaly Detection
  (<xref alt="Cazzonelli &amp; Kulbach, 2022" rid="ref-cazzonelli2022detecting" ref-type="bibr">Cazzonelli
  &amp; Kulbach, 2022</xref>). As the demand for effective and efficient
  adaptation of machine learning systems to evolving data structures
  continues to grow, the integration of <monospace>DeepRiver</monospace>
  into the landscape becomes crucial. This package stands as a valuable
  asset, unlocking the potential for deep learning technologies to excel
  in online, streaming, and continuous learning scenarios. The need for
  such advancements is evident in the quest to harness the full power of
  machine learning in dynamically changing environments, ensuring our
  decision-making processes remain accurate, relevant, and agile in the
  face of evolving data landscapes.</p>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>Online machine learning involves updating models incrementally as
  new data arrives, rather than retraining models from scratch. Several
  frameworks and libraries have been developed to support this
  paradigm:</p>
  <list list-type="bullet">
    <list-item>
      <p>scikit-multiflow
      (<xref alt="Montiel et al., 2018" rid="ref-10.5555U002F3291125.3309634" ref-type="bibr">Montiel
      et al., 2018</xref>)</p>
      <list list-type="bullet">
        <list-item>
          <p>Python-based Library: Inspired by the Java-based MOA
          framework, designed for streaming data and online learning in
          Python.</p>
        </list-item>
        <list-item>
          <p>Key Features:</p>
          <list list-type="bullet">
            <list-item>
              <p>Supports algorithms like Hoeffding Trees, online
              bagging, and boosting.</p>
            </list-item>
            <list-item>
              <p>Includes concept drift detection (e.g., ADWIN,
              Page-Hinkley) to adapt to changing data distributions.</p>
            </list-item>
            <list-item>
              <p>Stream generators and evaluators for real-time data
              simulation and model assessment.</p>
            </list-item>
          </list>
        </list-item>
        <list-item>
          <p>Limitations: Focuses mainly on traditional machine learning
          methods, with limited support for deep learning
          architectures.</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p>creme
      (<xref alt="Halford et al., 2020" rid="ref-creme" ref-type="bibr">Halford
      et al., 2020</xref>)</p>
      <list list-type="bullet">
        <list-item>
          <p>Lightweight Online Learning: Specialized in incremental
          learning where models are updated per instance, leading to
          efficient, low-latency model training.</p>
        </list-item>
        <list-item>
          <p>Provides a unified API with a broad range of online
          learning algorithms, making it the go-to library for streaming
          data analysis in Python. Limitations: Primarily supports
          feature-based models with limited capabilities for deep neural
          networks.</p>
        </list-item>
      </list>
    </list-item>
  </list>
  <p>In 2020, creme merged with scikit-multiflow to create River,
  combining the strengths of both frameworks.</p>
  <list list-type="bullet">
    <list-item>
      <p>Massive Online Analysis (MOA)
      (<xref alt="Bifet et al., 2010" rid="ref-10.5555U002F1756006.1859903" ref-type="bibr">Bifet
      et al., 2010</xref>)</p>
      <list list-type="bullet">
        <list-item>
          <p>Java-based Pioneer: One of the earliest frameworks
          dedicated to stream mining and online learning, widely used in
          academic research.</p>
        </list-item>
        <list-item>
          <p>Key Features:</p>
          <list list-type="bullet">
            <list-item>
              <p>Introduced foundational algorithms like Hoeffding
              Trees, Adaptive Random Forest (ARF), and several drift
              detection techniques (e.g., DDM, EDDM).</p>
            </list-item>
            <list-item>
              <p>Excellent scalability for handling high-throughput data
              streams in real-time.</p>
            </list-item>
            <list-item>
              <p>Strong focus on concept drift adaptation, making it
              robust in non-stationary environments.</p>
            </list-item>
          </list>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p>capyMOA(<xref alt="CapyMOA &amp;#X2014; CapyMOA — Capymoa.org" rid="ref-capymoaCapyMOAx2024" ref-type="bibr"><italic>CapyMOA
      &amp;#X2014; CapyMOA — Capymoa.org</italic></xref>)</p>
      <list list-type="bullet">
        <list-item>
          <p>Python Interface for MOA: capyMOA serves as a bridge
          between the Java-based MOA framework and Python, allowing
          users to leverage MOA’s powerful streaming algorithms within
          Python workflows.</p>
          <list list-type="bullet">
            <list-item>
              <p>Key Features:</p>
              <list list-type="bullet">
                <list-item>
                  <p>Enables access to MOA’s core functionalities (e.g.,
                  Hoeffding Trees, Adaptive Random Forest) from
                  Python.</p>
                </list-item>
                <list-item>
                  <p>Facilitates hybrid workflows by integrating MOA’s
                  Java algorithms with Python’s machine learning
                  libraries.</p>
                </list-item>
                <list-item>
                  <p>Useful for Python developers looking to use MOA’s
                  advanced stream mining capabilities without switching
                  ecosystems.</p>
                </list-item>
              </list>
            </list-item>
          </list>
        </list-item>
      </list>
    </list-item>
  </list>
  <p>scikit-multiflow and creme (River) focus on efficient online
  learning in Python, mainly for traditional machine learning
  algorithms. MOA offers extensive tools for stream mining but lacks
  deep learning support and Python compatibility. While capyMOA provides
  Python accessibility to MOA, capyMOA is limited by the underlying Java
  infrastructure and lacks a natural integration with PyTorch’s deep
  learning ecosystem.</p>
  <p><monospace>DeepRiver</monospace>(<xref alt="GitHub - Online-Ml/Deep-River — Github.com" rid="ref-githubGitHubOnlinemldeepriver" ref-type="bibr"><italic>GitHub
  - Online-Ml/Deep-River — Github.com</italic></xref>) differentiates
  itself by integrating deep learning capabilities directly into
  streaming data workflows, enabling continuous learning for neural
  network models. This addresses a critical gap left by existing
  frameworks, which are predominantly focused on non-deep learning
  models.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <p><monospace>DeepRiver</monospace>(<xref alt="GitHub - Online-Ml/Deep-River — Github.com" rid="ref-githubGitHubOnlinemldeepriver" ref-type="bibr"><italic>GitHub
  - Online-Ml/Deep-River — Github.com</italic></xref>) enables the usage
  of deep learning models for data streams. This means that deep
  learning models need to adapt to changes within the evolving data
  stream
  (<xref alt="Bayram et al., 2022" rid="ref-bayram2022concept" ref-type="bibr">Bayram
  et al., 2022</xref>;
  <xref alt="Lu et al., 2018" rid="ref-lu2018learning" ref-type="bibr">Lu
  et al., 2018</xref>) i.ex. the number of classes might change over
  time. In addition to the integration of
  PyTorch(<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>) into
  River(<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
  et al., 2021</xref>), this package offers additional data stream
  specific functionalities such as class incremental learning or
  specific optimizers for data streams.</p>
  <sec id="compatibility">
    <title>Compatibility</title>
    <p><monospace>DeepRiver</monospace>(<xref alt="GitHub - Online-Ml/Deep-River — Github.com" rid="ref-githubGitHubOnlinemldeepriver" ref-type="bibr"><italic>GitHub
    - Online-Ml/Deep-River — Github.com</italic></xref>) is built on the
    unified application programming interface (API) of
    River(<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
    et al., 2021</xref>) that seamlessly integrates both supervised and
    unsupervised learning techniques. Further, it integrates the huge
    functionality of
    PyTorch(<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
    et al., 2017</xref>) for deep learning such as using GPU
    acceleration and a broad range of architectures. This unified
    approach simplifies the development process and facilitates a
    cohesive workflow for practitioners working with dynamic data
    streams. Leveraging the capabilities of the well-established
    River(<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
    et al., 2021</xref>) library and the powerful
    PyTorch(<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
    et al., 2017</xref>) framework, <monospace>DeepRiver</monospace>
    combines the strengths of these technologies to deliver a robust and
    flexible platform for deep learning on data streams. This foundation
    ensures reliability, scalability, and compatibility with
    state-of-the-art machine learning methodologies. It provides
    comprehensive
    <ext-link ext-link-type="uri" xlink:href="https://online-ml.github.io/deep-river/">documentation</ext-link>
    to guide users through the installation, implementation, and
    customization processes. Additionally, a supportive community
    ensures that users have access to resources, discussions, and
    assistance, fostering a collaborative environment for continuous
    improvement and knowledge sharing.</p>
  </sec>
  <sec id="adaptivity">
    <title>Adaptivity</title>
    <p><monospace>DeepRiver</monospace>(<xref alt="GitHub - Online-Ml/Deep-River — Github.com" rid="ref-githubGitHubOnlinemldeepriver" ref-type="bibr"><italic>GitHub
    - Online-Ml/Deep-River — Github.com</italic></xref>) is specifically
    designed to cater to the requirements of online learning scenarios.
    It enables continuous adaptation to evolving data by supporting
    incremental updates and learning from new observations in real time,
    a critical feature for applications where data arrives sequentially.
    Further, it enables the model to adapt to changes in the number of
    classes over time for classification tasks. It equips practitioners
    with tools for evaluating the performance of deep learning models on
    data streams. This feature is crucial for ensuring the reliability
    and effectiveness of models in real-time applications, enabling
    users to monitor and fine-tune their models as the data evolves.</p>
  </sec>
</sec>
<sec id="architecture">
  <title>Architecture</title>
  <p>The <monospace>DeepRiver</monospace> library is structured around
  various types of estimators for anomaly detection, classification, and
  regression. Each category contains several specialized classes that
  inherit from more general base classes, forming a hierarchical
  structure. In anomaly detection, the base class
  <monospace>AnomalyScaler</monospace> has derived classes
  <monospace>AnomalyMeanScaler</monospace>,
  <monospace>AnomalyMinMaxScaler</monospace>, and
  <monospace>AnomalyStandardScaler</monospace>. Additionally, the
  <monospace>Autoencoder</monospace> class, which inherits from
  <monospace>DeepEstimator</monospace>, has a specialized subclass
  called <monospace>ProbabilityWeightedAutoencoder</monospace>. The
  <monospace>RollingAutoencoder</monospace> class inherits from
  <monospace>RollingDeepEstimator</monospace>.</p>
  <p>For classification, the base class
  <monospace>Classifier</monospace> inherits from
  <monospace>DeepEstimator</monospace>. Derived from
  <monospace>Classifier</monospace> are specific classes like
  <monospace>LogisticRegression</monospace> and
  <monospace>MultiLayerPerceptron</monospace>. The
  <monospace>RollingClassifier</monospace> class inherits from both
  <monospace>RollingDeepEstimator</monospace> and
  <monospace>Classifier</monospace>.</p>
  <p>In regression, the base class <monospace>Regressor</monospace>
  inherits from <monospace>DeepEstimator</monospace>. Specific
  regression classes like <monospace>LinearRegression</monospace> and
  <monospace>MultiLayerPerceptron</monospace> inherit from
  <monospace>Regressor</monospace>. The
  <monospace>MultiTargetRegressor</monospace> also inherits from
  <monospace>DeepEstimator</monospace>. The
  <monospace>RollingRegressor</monospace> class inherits from both
  <monospace>RollingDeepEstimator</monospace> and
  <monospace>Regressor</monospace>.</p>
  <fig>
    <caption><p>Architecture of
    DeepRiver<styled-content id="figU003AArchitecture"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="classes.png" />
  </fig>
  <p>Overall, the library is organized to provide a flexible and
  hierarchical framework for different types of machine learning tasks,
  with a clear inheritance structure connecting more specific
  implementations to their base classes.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work was carried out with the support of the Research Center
  for Information Technology in Karlsruhe, Germany.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-montiel2021river">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Montiel</surname><given-names>Jacob</given-names></name>
        <name><surname>Halford</surname><given-names>Max</given-names></name>
        <name><surname>Mastelini</surname><given-names>Saulo Martiello</given-names></name>
        <name><surname>Bolmier</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Sourty</surname><given-names>Raphael</given-names></name>
        <name><surname>Vaysse</surname><given-names>Robin</given-names></name>
        <name><surname>Zouitine</surname><given-names>Adil</given-names></name>
        <name><surname>Gomes</surname><given-names>Heitor Murilo</given-names></name>
        <name><surname>Read</surname><given-names>Jesse</given-names></name>
        <name><surname>Abdessalem</surname><given-names>Talel</given-names></name>
        <name><surname>Bifet</surname><given-names>Albert</given-names></name>
      </person-group>
      <article-title>River: Machine learning for streaming data in python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>110</issue>
      <uri>http://jmlr.org/papers/v22/20-1380.html</uri>
      <fpage>1</fpage>
      <lpage>8</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kulbach2024retrospectivetutorialopportunitieschallenges">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kulbach</surname><given-names>Cedric</given-names></name>
        <name><surname>Cazzonelli</surname><given-names>Lucas</given-names></name>
        <name><surname>Ngo</surname><given-names>Hoang-Anh</given-names></name>
        <name><surname>Le-Nguyen</surname><given-names>Minh-Huong</given-names></name>
        <name><surname>Bifet</surname><given-names>Albert</given-names></name>
      </person-group>
      <article-title>A retrospective of the tutorial on opportunities and challenges of online deep learning</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2405.17222</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2405.17222</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-cazzonelli2022detecting">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Cazzonelli</surname><given-names>Lucas</given-names></name>
        <name><surname>Kulbach</surname><given-names>Cedric</given-names></name>
      </person-group>
      <article-title>Detecting anomalies with autoencoders on data streams</article-title>
      <source>Joint european conference on machine learning and knowledge discovery in databases</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1007/978-3-031-26387-3_16</pub-id>
      <fpage>258</fpage>
      <lpage>274</lpage>
    </element-citation>
  </ref>
  <ref id="ref-paszke2017automatic">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
      </person-group>
      <article-title>Automatic differentiation in PyTorch</article-title>
      <source>NIPS-w</source>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-bayram2022concept">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bayram</surname><given-names>Firas</given-names></name>
        <name><surname>Ahmed</surname><given-names>Bestoun S</given-names></name>
        <name><surname>Kassler</surname><given-names>Andreas</given-names></name>
      </person-group>
      <article-title>From concept drift to model degradation: An overview on performance-aware drift detectors</article-title>
      <source>Knowledge-Based Systems</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>245</volume>
      <pub-id pub-id-type="doi">10.1016/j.knosys.2022.108632</pub-id>
      <fpage>108632</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lu2018learning">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lu</surname><given-names>Jie</given-names></name>
        <name><surname>Liu</surname><given-names>Anjin</given-names></name>
        <name><surname>Dong</surname><given-names>Fan</given-names></name>
        <name><surname>Gu</surname><given-names>Feng</given-names></name>
        <name><surname>Gama</surname><given-names>Joao</given-names></name>
        <name><surname>Zhang</surname><given-names>Guangquan</given-names></name>
      </person-group>
      <article-title>Learning under concept drift: A review</article-title>
      <source>IEEE transactions on knowledge and data engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1109/TKDE.2018.2876857</pub-id>
      <fpage>2346</fpage>
      <lpage>2363</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.5555U002F1756006.1859903">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bifet</surname><given-names>Albert</given-names></name>
        <name><surname>Holmes</surname><given-names>Geoff</given-names></name>
        <name><surname>Kirkby</surname><given-names>Richard</given-names></name>
        <name><surname>Pfahringer</surname><given-names>Bernhard</given-names></name>
      </person-group>
      <article-title>MOA: Massive online analysis</article-title>
      <source>J. Mach. Learn. Res.</source>
      <publisher-name>JMLR.org</publisher-name>
      <year iso-8601-date="2010-08">2010</year><month>08</month>
      <volume>11</volume>
      <issn>1532-4435</issn>
      <fpage>1601</fpage>
      <lpage>1604</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.5555U002F3291125.3309634">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Montiel</surname><given-names>Jacob</given-names></name>
        <name><surname>Read</surname><given-names>Jesse</given-names></name>
        <name><surname>Bifet</surname><given-names>Albert</given-names></name>
        <name><surname>Abdessalem</surname><given-names>Talel</given-names></name>
      </person-group>
      <article-title>Scikit-multiflow: A multi-output streaming framework</article-title>
      <source>J. Mach. Learn. Res.</source>
      <publisher-name>JMLR.org</publisher-name>
      <year iso-8601-date="2018-01">2018</year><month>01</month>
      <volume>19</volume>
      <issue>1</issue>
      <issn>1532-4435</issn>
      <fpage>2915</fpage>
      <lpage>2914</lpage>
    </element-citation>
  </ref>
  <ref id="ref-creme">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Halford</surname><given-names>Max</given-names></name>
        <name><surname>Bolmier</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Sourty</surname><given-names>Raphael</given-names></name>
        <name><surname>Vaysse</surname><given-names>Robin</given-names></name>
        <name><surname>Zouitine</surname><given-names>Adil</given-names></name>
      </person-group>
      <article-title>creme, a Python library for online machine learning</article-title>
      <year iso-8601-date="2020-06-10">2020</year><month>06</month><day>10</day>
      <uri>https://github.com/MaxHalford/creme</uri>
    </element-citation>
  </ref>
  <ref id="ref-capymoaCapyMOAx2024">
    <element-citation>
      <article-title>CapyMOA &amp;#x2014; CapyMOA — capymoa.org</article-title>
      <publisher-name>https://capymoa.org</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-githubGitHubOnlinemldeepriver">
    <element-citation>
      <article-title>GitHub - online-ml/deep-river — github.com</article-title>
      <publisher-name>https://github.com/online-ml/deep-river</publisher-name>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
